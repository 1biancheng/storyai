{
  "summary": {
    "total_tests": 17,
    "successful_tests": 10,
    "failed_tests": 7,
    "success_rate": 58.82
  },
  "test_results": [
    {
      "test_name": "Health Check",
      "method": "GET",
      "endpoint": "/",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 0.002,
      "success": true,
      "response_data": {
        "message": "StoryAI Backend API",
        "version": "1.0.0",
        "status": "running"
      }
    },
    {
      "test_name": "AI Health Check",
      "method": "GET",
      "endpoint": "/api/ai/health",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 4.084,
      "success": true,
      "response_data": {
        "status": "healthy",
        "ai_service": "running",
        "cache": {
          "l1_size": 0,
          "redis_available": true,
          "redis_connected": false
        }
      }
    },
    {
      "test_name": "Get Available Models",
      "method": "GET",
      "endpoint": "/api/ai/models",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 0.001,
      "success": true,
      "response_data": [
        {
          "model_id": "gpt-4",
          "provider": "openai",
          "model_name": "gpt-4",
          "available": false,
          "streaming_support": true,
          "max_tokens": 8192,
          "description": "Openai - gpt-4"
        },
        {
          "model_id": "gpt-4-turbo",
          "provider": "openai",
          "model_name": "gpt-4-turbo",
          "available": false,
          "streaming_support": true,
          "max_tokens": 4096,
          "description": "Openai - gpt-4-turbo"
        },
        {
          "model_id": "gpt-3.5-turbo",
          "provider": "openai",
          "model_name": "gpt-3.5-turbo",
          "available": false,
          "streaming_support": true,
          "max_tokens": 4096,
          "description": "Openai - gpt-3.5-turbo"
        },
        {
          "model_id": "azure-gpt-4",
          "provider": "openai",
          "model_name": "gpt-4",
          "available": false,
          "streaming_support": true,
          "max_tokens": 8192,
          "description": "Openai - gpt-4"
        },
        {
          "model_id": "moonshot-v1-8k",
          "provider": "openai",
          "model_name": "moonshot-v1-8k",
          "available": false,
          "streaming_support": true,
          "max_tokens": 8192,
          "description": "Openai - moonshot-v1-8k"
        },
        {
          "model_id": "moonshot-v1-32k",
          "provider": "openai",
          "model_name": "moonshot-v1-32k",
          "available": false,
          "streaming_support": true,
          "max_tokens": 32768,
          "description": "Openai - moonshot-v1-32k"
        },
        {
          "model_id": "deepseek-chat",
          "provider": "openai",
          "model_name": "deepseek-chat",
          "available": true,
          "streaming_support": true,
          "max_tokens": 4096,
          "description": "Openai - deepseek-chat"
        },
        {
          "model_id": "deepseek-coder",
          "provider": "openai",
          "model_name": "deepseek-coder",
          "available": true,
          "streaming_support": true,
          "max_tokens": 4096,
          "description": "Openai - deepseek-coder"
        },
        {
          "model_id": "gemini-pro",
          "provider": "gemini",
          "model_name": "gemini-pro",
          "available": false,
          "streaming_support": true,
          "max_tokens": 2048,
          "description": "Gemini - gemini-pro"
        },
        {
          "model_id": "gemini-1.5-pro",
          "provider": "gemini",
          "model_name": "gemini-1.5-pro",
          "available": false,
          "streaming_support": true,
          "max_tokens": 8192,
          "description": "Gemini - gemini-1.5-pro"
        },
        {
          "model_id": "claude-3-sonnet",
          "provider": "anthropic",
          "model_name": "claude-3-sonnet-20240229",
          "available": false,
          "streaming_support": true,
          "max_tokens": 4096,
          "description": "Anthropic - claude-3-sonnet-20240229"
        },
        {
          "model_id": "claude-3-haiku",
          "provider": "anthropic",
          "model_name": "claude-3-haiku-20240307",
          "available": false,
          "streaming_support": true,
          "max_tokens": 4096,
          "description": "Anthropic - claude-3-haiku-20240307"
        },
        {
          "model_id": "mistral-large",
          "provider": "openai",
          "model_name": "mistral-large-latest",
          "available": false,
          "streaming_support": true,
          "max_tokens": 4096,
          "description": "Openai - mistral-large-latest"
        }
      ]
    },
    {
      "test_name": "Get Model Config",
      "method": "GET",
      "endpoint": "/api/ai/models/gpt-3.5-turbo/config",
      "status_code": null,
      "expected_status": 200,
      "response_time": null,
      "success": false,
      "error": "Server disconnected"
    },
    {
      "test_name": "Cache Statistics",
      "method": "GET",
      "endpoint": "/api/ai/cache/stats",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 4.084,
      "success": true,
      "response_data": {
        "success": true,
        "stats": {
          "l1_cache": {
            "size": 0,
            "max_size": 512,
            "keys": []
          },
          "redis_available": true,
          "redis_connected": false
        }
      }
    },
    {
      "test_name": "Run AI Agent",
      "method": "POST",
      "endpoint": "/api/ai/run-agent",
      "status_code": 422,
      "expected_status": 200,
      "response_time": 0.001,
      "success": false,
      "response_data": {
        "detail": [
          {
            "type": "missing",
            "loc": [
              "body",
              "prompt"
            ],
            "msg": "Field required",
            "input": {
              "model_id": "gpt-3.5-turbo",
              "messages": [
                {
                  "role": "user",
                  "content": "Hello, this is a test message."
                }
              ],
              "max_tokens": 50
            }
          }
        ]
      }
    },
    {
      "test_name": "Test Model Connection",
      "method": "POST",
      "endpoint": "/api/ai/test-model",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 0.002,
      "success": true,
      "response_data": {
        "success": false,
        "model_id": "gpt-3.5-turbo",
        "error": "AIService.test_model_connection() got an unexpected keyword argument 'test_prompt'"
      }
    },
    {
      "test_name": "Database Statistics",
      "method": "GET",
      "endpoint": "/api/db/stats",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 8.344,
      "success": true,
      "response_data": {
        "success": true,
        "stats": {
          "total_documents": 0,
          "embedded_documents": 0,
          "embedding_coverage": 0,
          "vector_indexes": [],
          "database_url": "localhost:5432/story_ai"
        },
        "cached": false
      }
    },
    {
      "test_name": "Create Document",
      "method": "POST",
      "endpoint": "/api/db/documents",
      "status_code": 500,
      "expected_status": 201,
      "response_time": 0.268,
      "success": false,
      "response_data": {
        "success": false,
        "error": {
          "code": 500,
          "message": "(sqlalchemy.dialects.postgresql.asyncpg.ProgrammingError) <class 'asyncpg.exceptions.DatatypeMismatchError'>: 字段 \"doc_metadata\" 的类型为 jsonb, 但表达式的类型为 character varying\nHINT:  你需要重写或转换表达式\n[SQL: INSERT INTO documents (title, content, content_type, source, doc_metadata, embedding, embedding_model, updated_at, is_active) VALUES ($1::VARCHAR, $2::VARCHAR, $3::VARCHAR, $4::VARCHAR, $5::VARCHAR, $6::FLOAT[], $7::VARCHAR, $8::TIMESTAMP WITH TIME ZONE, $9::BOOLEAN) RETURNING documents.id, documents.created_at]\n[parameters: ('Test Document', 'This is a test document for API testing.', 'text', 'api_test', '{\"test\": true, \"created_by\": \"api_tester\"}', None, 'text-embedding-ada-002', None, True)]\n(Background on this error at: https://sqlalche.me/e/20/f405)"
        }
      }
    },
    {
      "test_name": "Vector Search",
      "method": "POST",
      "endpoint": "/api/db/search/vector",
      "status_code": 500,
      "expected_status": 200,
      "response_time": 4.266,
      "success": false,
      "response_data": {
        "success": false,
        "error": {
          "code": 500,
          "message": "(sqlalchemy.dialects.postgresql.asyncpg.ProgrammingError) <class 'asyncpg.exceptions.AmbiguousParameterError'>: 无法确定参数 $2 的数据类型\n[SQL: \n                SELECT id, title, content, content_type, source, doc_metadata,\n                       1 - (embedding <=> $1) as similarity\n                FROM documents \n                WHERE embedding IS NOT NULL \n                  AND is_active = true\n                  AND ($2 IS NULL OR content_type = $2)\n                  AND 1 - (embedding <=> $1) >= $3\n                ORDER BY embedding <=> $1\n                LIMIT $4\n            ]\n[parameters: ([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ... (7380 characters truncated) ...  0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], None, 0.5, 5)]\n(Background on this error at: https://sqlalche.me/e/20/f405)"
        }
      }
    },
    {
      "test_name": "Hybrid Search",
      "method": "POST",
      "endpoint": "/api/db/search/hybrid",
      "status_code": 500,
      "expected_status": 200,
      "response_time": 0.149,
      "success": false,
      "response_data": {
        "success": false,
        "error": {
          "code": 500,
          "message": "(sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.DataError'>: invalid input for query argument $2: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,... (expected str, got list)\n[SQL: \n                SELECT id, title, content, content_type, source, doc_metadata,\n                       ts_rank(to_tsvector('english', title || ' ' || content), plainto_tsquery($1)) as text_score,\n                       1 - (embedding <=> $2) as vector_score,\n                       ($3 * ts_rank(to_tsvector('english', title || ' ' || content), plainto_tsquery($1)) + \n                        $4 * (1 - (embedding <=> $2))) as combined_score\n                FROM documents \n                WHERE embedding IS NOT NULL \n                  AND is_active = true\n                  AND (to_tsvector('english', title || ' ' || content) @@ plainto_tsquery($1)\n                       OR 1 - (embedding <=> $2) >= 0.5)\n                ORDER BY combined_score DESC\n                LIMIT $5\n            ]\n[parameters: ('test document', [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ... (7380 characters truncated) ...  0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], 0.3, 0.7, 5)]\n(Background on this error at: https://sqlalche.me/e/20/dbapi)"
        }
      }
    },
    {
      "test_name": "Clear Cache",
      "method": "POST",
      "endpoint": "/api/ai/cache/clear",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 4.082,
      "success": true,
      "response_data": {
        "success": true,
        "message": "All cache cleared, 0 items removed"
      }
    },
    {
      "test_name": "Cache Stats After Clear",
      "method": "GET",
      "endpoint": "/api/ai/cache/stats",
      "status_code": 200,
      "expected_status": 200,
      "response_time": 4.099,
      "success": true,
      "response_data": {
        "success": true,
        "stats": {
          "l1_cache": {
            "size": 0,
            "max_size": 512,
            "keys": []
          },
          "redis_available": true,
          "redis_connected": false
        }
      }
    },
    {
      "test_name": "Non-existent Endpoint",
      "method": "GET",
      "endpoint": "/api/nonexistent",
      "status_code": 404,
      "expected_status": 404,
      "response_time": 0.001,
      "success": true,
      "response_data": {
        "detail": "Not Found"
      }
    },
    {
      "test_name": "Invalid Model Config",
      "method": "GET",
      "endpoint": "/api/ai/models/invalid-model/config",
      "status_code": null,
      "expected_status": 404,
      "response_time": null,
      "success": false,
      "error": "Server disconnected"
    },
    {
      "test_name": "Invalid Document ID",
      "method": "GET",
      "endpoint": "/api/db/documents/99999",
      "status_code": 500,
      "expected_status": 404,
      "response_time": 4.24,
      "success": false,
      "response_data": {
        "success": false,
        "error": {
          "code": 500,
          "message": "Unknown PG numeric type: 24578"
        }
      }
    },
    {
      "test_name": "Invalid Agent Request",
      "method": "POST",
      "endpoint": "/api/ai/run-agent",
      "status_code": 422,
      "expected_status": 422,
      "response_time": 0.001,
      "success": true,
      "response_data": {
        "detail": [
          {
            "type": "missing",
            "loc": [
              "body",
              "prompt"
            ],
            "msg": "Field required",
            "input": {
              "invalid": "data"
            }
          }
        ]
      }
    }
  ]
}